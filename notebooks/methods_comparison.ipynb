{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods comparison\n",
    "- SVRG (with/without outer loop)\n",
    "- Katyusha (with/without outer loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Problem:\n",
    "    def get_value(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_gradient(self, *args, **kwargs):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Logistic Regression (BLR) problem can be defined as follows:\n",
    "$$\\begin{equation}\n",
    "\\min_{w \\in \\mathbb{R}^d} \\frac{1}{n} \\sum\\limits_{i=1}^n \\ell (g(w, x_i), y_i) + \\frac{\\lambda}{2} \\| w \\|^2_2,\n",
    "\\end{equation}\n",
    "$$\n",
    "where $\\ell(z,y) = \\ln (1 + e^{-yz})$ is the loss function, $g(w, x) = w^T x$ is the model, $w$ is the model parameters, $\\{x_i, y_i\\}_{i=1}^n$ is the data sample from feature vectors $x_i$ and labels $y_i$, $\\lambda > 0$ is the regularization parameter.\n",
    "\n",
    "**Important Assumption**: $y$ must take values $-1$ or $+1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma 1**\n",
    "Let $x \\in \\mathbb{R}^d$. Then $X=xx^T \\succeq 0$.\n",
    "\n",
    "**Proof**\n",
    "Let $y \\in \\mathbb{R}^d$ be column vector. Then\n",
    "$$\n",
    "y^TXy = y^Txx^Ty = (x^Ty)^T(x^Ty) = \\|x^Ty\\|_2^2 \\geq 0\n",
    "$$\n",
    "Therefore, $X=xx^T$ is positive semi-definite by definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma 2**\n",
    "Let $x \\in \\mathbb{R}^d$. Then $(x^Tx)I_n \\succeq xx^T$.\n",
    "\n",
    "**Proof**\n",
    "Let us denote $X =xx^T$, $r = x^Tx = \\|x\\|_2^2$ and $X' = X-rI_n$.  \n",
    "Now let's look on calculation of eigenvalues for matrix $X'$.\n",
    "We will have to calculate determinant of $X'-\\lambda I_n = X-r I_n - \\lambda I_n = X-(r + \\lambda)I$.  \n",
    "So we can claim that eigenvalues of $X'$ are just eigenvalues of $X$ minus $r$ (let us denote this fact as $eig(X')=eig(X)-r$).  \n",
    "Then also recap property of definite matrix: matrix is positive definite if and only if all of its eigenvalues are positive.\n",
    "\n",
    "Therefore, we need to prove that eigenvalues of $-X' = rI_n - X$ are nonnegative, what means that eigenvalues of $X' = X - rI_n $ are non-positive.  \n",
    "\n",
    "It is sufficient to show that the maximum eigenvalue $eig_{max}(X')$ is non-positive. We have shown that $eig(X')=eig(X)-r$.  \n",
    "As $r \\geq 0$, $eig_{max}(X') = eig_{max}(X') - r$.  \n",
    "Note that $Sum(eig(X))=Tr(X)= r$, and, by *Lemma 1*, $X \\succeq 0$, so all eigenvalue of $X$ are nonnegative.  \n",
    "Therefore, the maximum possible $eig_{max}(X') \\leq r$ and $eig_{max}(X') = eig_{max}(X') - r \\leq 0$.  \n",
    "\n",
    "We has proven that $eig_{max}(X') \\leq 0$, so $eig(X') \\leq 0$ and $eig(-X') \\geq 0$. Therefore, $-X' = (x^Tx)I_n - xx^T \\succeq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the function $f$ as\n",
    "$$ f = \\frac{1}{n} \\sum\\limits_{i=1}^n \\ell (g(w, x_i), y_i) + \\frac{\\lambda}{2} \\| w \\|^2_2$$\n",
    "\n",
    "So the initial problem is minimizing $f$. Let us also use the following notations:\n",
    "$$\n",
    "e_i = e^{-y_iw^Tx_i} \\\\\n",
    "h_i(w) = \\ell (g(w, x_i), y_i) = ln(1+e^{-y_iw^Tx_i}) = ln(1+e_i) \\\\\n",
    "r(w) = \\frac{\\lambda}{2} \\| w \\|^2_2 = \\frac{\\lambda}{2} w^Tw\n",
    "$$\n",
    "\n",
    "To compute $\\nabla_w f$ and $\\nabla_w^2 f$, we first need to find $\\nabla h_i$, $\\nabla^2 h_i$, $\\nabla r$ and $\\nabla^2 r$.\n",
    "Note that $e_i' = -y_ie_ix_i$.\n",
    "\n",
    "Let us start with $h(w)$:\n",
    "$$\n",
    "\\nabla h_i = \\frac{-y_ie_ix_i}{1+e_i} \\\\\n",
    "\\nabla^2 h_i = \\frac{1}{(1+e_i)^2} (-y_i(-y_ie_ix_i)x_i(1+e_i)-(-y_ie_ix_i)(-y_ie_ix_i)) = \\\\\n",
    "= \\frac{y_i^2e_ix_ix_i^T}{(1+e_i)^2} (1+e_i-e_i) =  \\{ y_i^2 = 1 \\text{ as } y_i = \\pm 1 \\} = \\frac{e_ix_ix_i^T}{(1+e_i)^2}\n",
    "$$\n",
    "\n",
    "For $r(w)$,\n",
    "$$\n",
    "\\nabla r = \\lambda w\\\\\n",
    "\\nabla^2 r = \\lambda I_n\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "$$\n",
    "\\nabla f = \\frac{1}{n} \\sum\\limits_{i=1}^n \\nabla h_i(w) +  \\nabla r(w) = \\frac{1}{n} \\sum\\limits_{i=1}^n \\frac{-y_ie_ix_i}{1+e_i} +  \\lambda w\\\\\n",
    "\n",
    "\\nabla^2 f = \\frac{1}{n} \\sum\\limits_{i=1}^n \\nabla^2 h_i(w) + \\nabla^2 r(w) =\\frac{1}{n} \\sum\\limits_{i=1}^n \\frac{e_ix_ix_i^T}{(1+e_i)^2} + \\lambda I_n\n",
    "$$\n",
    "\n",
    "From Theorem 2.1.6 and Theorem 2.1.11 from Nesterov's book (check references) we know that $f$ is $\\mu$-strongly convex and has $L$-Lipschitz gradient iff\n",
    "$$\n",
    "L  I_n \\succeq \\nabla^2 f \\succeq \\mu I_n\n",
    "$$\n",
    "\n",
    "Note that $e_i > 0$, so $\\frac{1}{n} \\sum\\limits_{i=1}^n \\frac{e_ix_ix_i^T}{(1+e_i)^2} + \\lambda I_n \\succeq \\lambda I_n$  (using *Lemma 1*).\n",
    "Therefore $\\nabla^2 f \\succeq \\lambda I_n$ and $f$ is $\\mu$-strongly convex with $\\mu = \\lambda$.\n",
    "\n",
    "Let us prove that $f$ has $L$-Lipschitz gradient with $L = \\lambda + \\frac{1}{4n} \\sum_{i=1}^n x_i^T x_i$. To do this we need to show that\n",
    "$$\n",
    "(\\lambda + \\frac{1}{4n} \\sum_{i=1}^n x_i^T x_i)I_n \\succeq \\frac{1}{n} \\sum\\limits_{i=1}^n \\frac{e_ix_ix_i^T}{(1+e_i)^2} + \\lambda I_n \\\\\n",
    "\\Longleftrightarrow \\\\\n",
    "(\\frac{1}{4n} \\sum_{i=1}^n x_i^T x_i)I_n \\succeq \\frac{1}{n} \\sum\\limits_{i=1}^n \\frac{e_ix_ix_i^T}{(1+e_i)^2}\n",
    "$$\n",
    "\n",
    "Let us focus on some fixed $x_i$ and prove\n",
    "$$(\\frac{1}{4n} x_i^T x_i)I_n \\succeq \\frac{1}{n} \\frac{e_ix_ix_i^T}{(1+e_i)^2}$$\n",
    "\n",
    "Using the fact that $x_i^T x_i I_n \\succeq  x_ix_i^T$ from *Lemma 2*, we can compare only matrix scalars and proceed with following\n",
    "$$\n",
    "\\frac{1}{4n} \\geq \\frac{1}{n} \\frac{e_i}{(1+e_i)^2} \\\\\n",
    "\\frac{1}{4} \\geq  \\frac{e_i}{(1+e_i)^2} \\\\\n",
    "(1+e_i)^2 \\geq 4e_i \\\\\n",
    "(1-e_i)^2 \\geq 0 \\\\\n",
    "$$\n",
    "\n",
    "The last statement is true for all $x_i$, so $L I_n \\succeq \\nabla^2 f$ is also true and $f$ has $L$-Lipschitz gradient with $L = \\lambda + \\frac{1}{4n} \\sum_{i=1}^n x_i^T x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "- $\\nabla f = \\frac{1}{n} \\sum\\limits_{i=1}^n \\frac{-y_ie_ix_i}{1+e_i} +  \\lambda w$\n",
    "- The problem is $\\mu$-strongly convex with $\\mu = \\lambda$\n",
    "- The problem has $L$-Lipschitz gradient $L = \\lambda + \\frac{1}{4n} \\sum_{i=1}^n \\| x_i\\|^2_2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression(Problem):\n",
    "    def _expand_dim(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Convert (n,) vector to (n,1)\"\"\"\n",
    "        return np.expand_dims(x, axis=1)\n",
    "\n",
    "    def _get_iterate_data(\n",
    "        self, indices: Optional[list[int]] = None\n",
    "    ) -> list[tuple[np.ndarray, float]]:\n",
    "        if indices == None:\n",
    "            return self.data\n",
    "        data = []\n",
    "        for idx in indices:\n",
    "            data.append(self.data[idx])\n",
    "        return data\n",
    "\n",
    "    def _custom_exponent(self, x: np.ndarray, y: float, w: np.ndarray) -> float:\n",
    "        return np.exp(-y * w.dot(x))\n",
    "\n",
    "    def __init__(\n",
    "        self, xs: np.ndarray, ys: np.ndarray, lambda_term: float, seed: float = 42\n",
    "    ) -> None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "        self.lambda_term = lambda_term\n",
    "\n",
    "        self.data = list(zip(self.xs, self.ys))\n",
    "\n",
    "        self.grad_shape = self.xs.shape[1]\n",
    "        self.hes_shape = (self.xs.shape[1], self.xs.shape[1])\n",
    "        self.sample_size = self.ys.shape[0]\n",
    "\n",
    "        self.identity = np.identity(self.grad_shape)\n",
    "\n",
    "    def get_uniformly_sampled_indices(self, n: int = 1) -> list[int]:\n",
    "        return [np.random.randint(0, len(self.data)) for _ in range(n)]\n",
    "\n",
    "    def get_value(self, w: np.ndarray, indices: Optional[list[int]] = None) -> float:\n",
    "        sum_result = 0\n",
    "\n",
    "        for x, y in self._get_iterate_data(indices):\n",
    "            sum_result += np.log(1 + self._custom_exponent(x, y, w))\n",
    "\n",
    "        squared_norm: float = w.dot(w)\n",
    "\n",
    "        return sum_result / self.sample_size + squared_norm * self.lambda_term / 2\n",
    "\n",
    "    def get_gradient(\n",
    "        self, w: np.ndarray, indices: Optional[list[int]] = None\n",
    "    ) -> np.ndarray:\n",
    "        sum_result = np.zeros(self.grad_shape)\n",
    "        for x, y in self._get_iterate_data(indices):\n",
    "            custom_exp = self._custom_exponent(x, y, w)\n",
    "            sum_result += (-y * custom_exp * x) / (1 + custom_exp)\n",
    "\n",
    "        return sum_result / self.sample_size + self.lambda_term * w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omml-project-f23-j1-kYpQ4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
