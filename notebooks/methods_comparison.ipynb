{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods comparison\n",
    "- SVRG (with/without outer loop)\n",
    "- Katyusha (with/without outer loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import time\n",
    "from itertools import accumulate\n",
    "from typing import Callable, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cycler import cycler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MANUAL_SEED = 42\n",
    "np.random.seed(MANUAL_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading & preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset reference can be found in README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(heart_df)=303\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trtbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalachh</th>\n",
       "      <th>exng</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slp</th>\n",
       "      <th>caa</th>\n",
       "      <th>thall</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trtbps  chol  fbs  restecg  thalachh  exng  oldpeak  slp  \\\n",
       "0   63    1   3     145   233    1        0       150     0      2.3    0   \n",
       "1   37    1   2     130   250    0        1       187     0      3.5    0   \n",
       "2   41    0   1     130   204    0        0       172     0      1.4    2   \n",
       "3   56    1   1     120   236    0        1       178     0      0.8    2   \n",
       "4   57    0   0     120   354    0        1       163     1      0.6    2   \n",
       "\n",
       "   caa  thall  output  \n",
       "0    0      1       1  \n",
       "1    0      2       1  \n",
       "2    0      2       1  \n",
       "3    0      2       1  \n",
       "4    0      2       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heart_df = pd.read_csv(\"../data/raw/heart_attack_dataset.csv\")\n",
    "print(f\"{len(heart_df)=}\")\n",
    "heart_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trtbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalachh</th>\n",
       "      <th>exng</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slp</th>\n",
       "      <th>caa</th>\n",
       "      <th>thall</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.818182</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.413121</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.742574</td>\n",
       "      <td>0</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.480519</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.443262</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.925743</td>\n",
       "      <td>0</td>\n",
       "      <td>0.564516</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.532468</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.361702</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.851485</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.727273</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.418440</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.881188</td>\n",
       "      <td>0</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.740260</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.627660</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.806931</td>\n",
       "      <td>1</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  sex  cp  trtbps      chol  fbs  restecg  thalachh  exng  \\\n",
       "0  0.818182    1   3   0.725  0.413121    1        0  0.742574     0   \n",
       "1  0.480519    1   2   0.650  0.443262    0        1  0.925743     0   \n",
       "2  0.532468    0   1   0.650  0.361702    0        0  0.851485     0   \n",
       "3  0.727273    1   1   0.600  0.418440    0        1  0.881188     0   \n",
       "4  0.740260    0   0   0.600  0.627660    0        1  0.806931     1   \n",
       "\n",
       "    oldpeak  slp  caa  thall  output  \n",
       "0  0.370968    0    0      1       1  \n",
       "1  0.564516    0    0      2       1  \n",
       "2  0.225806    2    0      2       1  \n",
       "3  0.129032    2    0      2       1  \n",
       "4  0.096774    2    0      2       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    new_df = df.copy()\n",
    "\n",
    "    # Make labels be 1 or -1\n",
    "    new_df[\"output\"] = new_df[\"output\"] * 2 - 1\n",
    "\n",
    "    # Normalize columns\n",
    "    new_df[\"age\"] = new_df[\"age\"] / new_df[\"age\"].max()\n",
    "    new_df[\"trtbps\"] = new_df[\"trtbps\"] / new_df[\"trtbps\"].max()\n",
    "    new_df[\"chol\"] = new_df[\"chol\"] / new_df[\"chol\"].max()\n",
    "    new_df[\"thalachh\"] = new_df[\"thalachh\"] / new_df[\"thalachh\"].max()\n",
    "    new_df[\"oldpeak\"] = new_df[\"oldpeak\"] / new_df[\"oldpeak\"].max()\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "preprocessed_df = preprocess_dataset(heart_df)\n",
    "preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "values.shape=(303, 13)\n",
      "targets.shape=(303,)\n"
     ]
    }
   ],
   "source": [
    "values = heart_df.drop(columns=[\"output\"]).to_numpy()\n",
    "targets = heart_df[\"output\"].to_numpy()\n",
    "print(f\"{values.shape=}\")\n",
    "print(f\"{targets.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.1\n",
    "\n",
    "test_length = int(test_ratio * len(targets))\n",
    "train_length = len(targets) - test_length\n",
    "\n",
    "indices = np.random.permutation(targets.shape[0])\n",
    "train_idx, test_idx = indices[:train_length], indices[train_length:]\n",
    "\n",
    "train_values, test_values = values[train_idx, :], values[test_idx, :]\n",
    "train_targets, test_targets = targets[train_idx], targets[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_values)=273\n",
      "len(train_targets)=273\n",
      "len(test_values)=30\n",
      "len(test_targets)=30\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_values)=}\")\n",
    "print(f\"{len(train_targets)=}\")\n",
    "print(f\"{len(test_values)=}\")\n",
    "print(f\"{len(test_targets)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Problem:\n",
    "    def get_uniformly_sampled_indices(self, n: int = 1) -> list[int]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_value(self, *args, **kwargs):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_gradient(self, *args, **kwargs):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Logistic Regression (BLR) problem can be defined as follows:\n",
    "$$\\begin{equation}\n",
    "\\min_{w \\in \\mathbb{R}^d} \\frac{1}{n} \\sum\\limits_{i=1}^n \\ell (g(w, x_i), y_i) + \\frac{\\lambda}{2} \\| w \\|^2_2,\n",
    "\\end{equation}\n",
    "$$\n",
    "where $\\ell(z,y) = \\ln (1 + e^{-yz})$ is the loss function, $g(w, x) = w^T x$ is the model, $w$ is the model parameters, $\\{x_i, y_i\\}_{i=1}^n$ is the data sample from feature vectors $x_i$ and labels $y_i$, $\\lambda > 0$ is the regularization parameter.\n",
    "\n",
    "**Important Assumption**: $y$ must take values $-1$ or $+1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma 1**\n",
    "Let $x \\in \\mathbb{R}^d$. Then $X=xx^T \\succeq 0$.\n",
    "\n",
    "**Proof**\n",
    "Let $y \\in \\mathbb{R}^d$ be column vector. Then\n",
    "$$\n",
    "y^TXy = y^Txx^Ty = (x^Ty)^T(x^Ty) = \\|x^Ty\\|_2^2 \\geq 0\n",
    "$$\n",
    "Therefore, $X=xx^T$ is positive semi-definite by definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma 2**\n",
    "Let $x \\in \\mathbb{R}^d$. Then $(x^Tx)I_n \\succeq xx^T$.\n",
    "\n",
    "**Proof**\n",
    "Let us denote $X =xx^T$, $r = x^Tx = \\|x\\|_2^2$ and $X' = X-rI_n$.  \n",
    "Now let's look on calculation of eigenvalues for matrix $X'$.\n",
    "We will have to calculate determinant of $X'-\\lambda I_n = X-r I_n - \\lambda I_n = X-(r + \\lambda)I$.  \n",
    "So we can claim that eigenvalues of $X'$ are just eigenvalues of $X$ minus $r$ (let us denote this fact as $eig(X')=eig(X)-r$).  \n",
    "Then also recap property of definite matrix: matrix is positive definite if and only if all of its eigenvalues are positive.\n",
    "\n",
    "Therefore, we need to prove that eigenvalues of $-X' = rI_n - X$ are nonnegative, what means that eigenvalues of $X' = X - rI_n $ are non-positive.  \n",
    "\n",
    "It is sufficient to show that the maximum eigenvalue $eig_{max}(X')$ is non-positive. We have shown that $eig(X')=eig(X)-r$.  \n",
    "As $r \\geq 0$, $eig_{max}(X') = eig_{max}(X') - r$.  \n",
    "Note that $Sum(eig(X))=Tr(X)= r$, and, by *Lemma 1*, $X \\succeq 0$, so all eigenvalue of $X$ are nonnegative.  \n",
    "Therefore, the maximum possible $eig_{max}(X') \\leq r$ and $eig_{max}(X') = eig_{max}(X') - r \\leq 0$.  \n",
    "\n",
    "We has proven that $eig_{max}(X') \\leq 0$, so $eig(X') \\leq 0$ and $eig(-X') \\geq 0$. Therefore, $-X' = (x^Tx)I_n - xx^T \\succeq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the function $f$ as\n",
    "$$ f = \\frac{1}{n} \\sum\\limits_{i=1}^n \\ell (g(w, x_i), y_i) + \\frac{\\lambda}{2} \\| w \\|^2_2$$\n",
    "\n",
    "So the initial problem is minimizing $f$. Let us also use the following notations:\n",
    "$$\n",
    "e_i = e^{-y_iw^Tx_i} \\\\\n",
    "h_i(w) = \\ell (g(w, x_i), y_i) = ln(1+e^{-y_iw^Tx_i}) = ln(1+e_i) \\\\\n",
    "r(w) = \\frac{\\lambda}{2} \\| w \\|^2_2 = \\frac{\\lambda}{2} w^Tw\n",
    "$$\n",
    "\n",
    "To compute $\\nabla_w f$ and $\\nabla_w^2 f$, we first need to find $\\nabla h_i$, $\\nabla^2 h_i$, $\\nabla r$ and $\\nabla^2 r$.\n",
    "Note that $e_i' = -y_ie_ix_i$.\n",
    "\n",
    "Let us start with $h(w)$:\n",
    "$$\n",
    "\\nabla h_i = \\frac{-y_ie_ix_i}{1+e_i} \\\\\n",
    "\\nabla^2 h_i = \\frac{1}{(1+e_i)^2} (-y_i(-y_ie_ix_i)x_i(1+e_i)-(-y_ie_ix_i)(-y_ie_ix_i)) = \\\\\n",
    "= \\frac{y_i^2e_ix_ix_i^T}{(1+e_i)^2} (1+e_i-e_i) =  \\{ y_i^2 = 1 \\text{ as } y_i = \\pm 1 \\} = \\frac{e_ix_ix_i^T}{(1+e_i)^2}\n",
    "$$\n",
    "\n",
    "For $r(w)$,\n",
    "$$\n",
    "\\nabla r = \\lambda w\\\\\n",
    "\\nabla^2 r = \\lambda I_n\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "$$\n",
    "\\nabla f = \\frac{1}{n} \\sum\\limits_{i=1}^n \\nabla h_i(w) +  \\nabla r(w) = \\frac{1}{n} \\sum\\limits_{i=1}^n \\frac{-y_ie_ix_i}{1+e_i} +  \\lambda w\\\\\n",
    "\n",
    "\\nabla^2 f = \\frac{1}{n} \\sum\\limits_{i=1}^n \\nabla^2 h_i(w) + \\nabla^2 r(w) =\\frac{1}{n} \\sum\\limits_{i=1}^n \\frac{e_ix_ix_i^T}{(1+e_i)^2} + \\lambda I_n\n",
    "$$\n",
    "\n",
    "From Theorem 2.1.6 and Theorem 2.1.11 from Nesterov's book (check references) we know that $f$ is $\\mu$-strongly convex and has $L$-Lipschitz gradient iff\n",
    "$$\n",
    "L  I_n \\succeq \\nabla^2 f \\succeq \\mu I_n\n",
    "$$\n",
    "\n",
    "Note that $e_i > 0$, so $\\frac{1}{n} \\sum\\limits_{i=1}^n \\frac{e_ix_ix_i^T}{(1+e_i)^2} + \\lambda I_n \\succeq \\lambda I_n$  (using *Lemma 1*).\n",
    "Therefore $\\nabla^2 f \\succeq \\lambda I_n$ and $f$ is $\\mu$-strongly convex with $\\mu = \\lambda$.\n",
    "\n",
    "Let us prove that $f$ has $L$-Lipschitz gradient with $L = \\lambda + \\frac{1}{4n} \\sum_{i=1}^n x_i^T x_i$. To do this we need to show that\n",
    "$$\n",
    "(\\lambda + \\frac{1}{4n} \\sum_{i=1}^n x_i^T x_i)I_n \\succeq \\frac{1}{n} \\sum\\limits_{i=1}^n \\frac{e_ix_ix_i^T}{(1+e_i)^2} + \\lambda I_n \\\\\n",
    "\\Longleftrightarrow \\\\\n",
    "(\\frac{1}{4n} \\sum_{i=1}^n x_i^T x_i)I_n \\succeq \\frac{1}{n} \\sum\\limits_{i=1}^n \\frac{e_ix_ix_i^T}{(1+e_i)^2}\n",
    "$$\n",
    "\n",
    "Let us focus on some fixed $x_i$ and prove\n",
    "$$(\\frac{1}{4n} x_i^T x_i)I_n \\succeq \\frac{1}{n} \\frac{e_ix_ix_i^T}{(1+e_i)^2}$$\n",
    "\n",
    "Using the fact that $x_i^T x_i I_n \\succeq  x_ix_i^T$ from *Lemma 2*, we can compare only matrix scalars and proceed with following\n",
    "$$\n",
    "\\frac{1}{4n} \\geq \\frac{1}{n} \\frac{e_i}{(1+e_i)^2} \\\\\n",
    "\\frac{1}{4} \\geq  \\frac{e_i}{(1+e_i)^2} \\\\\n",
    "(1+e_i)^2 \\geq 4e_i \\\\\n",
    "(1-e_i)^2 \\geq 0 \\\\\n",
    "$$\n",
    "\n",
    "The last statement is true for all $x_i$, so $L I_n \\succeq \\nabla^2 f$ is also true and $f$ has $L$-Lipschitz gradient with $L = \\lambda + \\frac{1}{4n} \\sum_{i=1}^n x_i^T x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "- $\\nabla f = \\frac{1}{n} \\sum\\limits_{i=1}^n \\frac{-y_ie_ix_i}{1+e_i} +  \\lambda w$\n",
    "- The problem is $\\mu$-strongly convex with $\\mu = \\lambda$\n",
    "- The problem has $L$-Lipschitz gradient $L = \\lambda + \\frac{1}{4n} \\sum_{i=1}^n \\| x_i\\|^2_2$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryLogisticRegression(Problem):\n",
    "    @staticmethod\n",
    "    def calculate_parameters(\n",
    "        lambda_ratio: float, x_values: np.ndarray\n",
    "    ) -> tuple[float, float, float]:\n",
    "        \"\"\"Calculate required parameters\n",
    "        for the binary logistic regression problem\n",
    "\n",
    "        Args:\n",
    "            lambda_ratio (float): defined as `lambda_term = lambda_ratio * lipschitz`\n",
    "            x_values (np.ndarray): training part\n",
    "\n",
    "        Returns:\n",
    "            tuple[float,float,float]: (lambda, mu, lipschitz)\n",
    "        \"\"\"\n",
    "\n",
    "        products_sum = 0\n",
    "        for x in x_values:\n",
    "            products_sum += x @ x\n",
    "\n",
    "        lipschitz = float(products_sum / (4 * x_values.shape[0] * (1 - lambda_ratio)))\n",
    "        lambda_term = lambda_ratio * lipschitz\n",
    "        mu = lambda_term\n",
    "\n",
    "        return lambda_term, mu, lipschitz\n",
    "\n",
    "    def _expand_dim(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Convert (n,) vector to (n,1)\"\"\"\n",
    "        return np.expand_dims(x, axis=1)\n",
    "\n",
    "    def _get_iterate_data(\n",
    "        self, indices: Optional[list[int]] = None\n",
    "    ) -> list[tuple[np.ndarray, float]]:\n",
    "        if indices is None:\n",
    "            return self.data\n",
    "        return [self.data[idx] for idx in indices]\n",
    "\n",
    "    def _custom_exponent(self, x: np.ndarray, y: float, w: np.ndarray) -> float:\n",
    "        return np.exp(-y * w.dot(x))\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        xs: np.ndarray,\n",
    "        ys: np.ndarray,\n",
    "        lambda_term: float,\n",
    "        seed: float = MANUAL_SEED,\n",
    "    ) -> None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "        self.lambda_term = lambda_term\n",
    "\n",
    "        self.data = list(zip(self.xs, self.ys))\n",
    "\n",
    "        self.grad_shape = self.xs.shape[1]\n",
    "        self.hes_shape = (self.xs.shape[1], self.xs.shape[1])\n",
    "        self.sample_size = self.ys.shape[0]\n",
    "\n",
    "        self.identity = np.identity(self.grad_shape)\n",
    "\n",
    "    def get_uniformly_sampled_indices(self, n: int = 1) -> list[int]:\n",
    "        return [np.random.randint(0, len(self.data)) for _ in range(n)]\n",
    "\n",
    "    def get_value(self, w: np.ndarray, indices: Optional[list[int]] = None) -> float:\n",
    "        sum_result = 0\n",
    "\n",
    "        for x, y in self._get_iterate_data(indices):\n",
    "            sum_result += np.log(1 + self._custom_exponent(x, y, w))\n",
    "\n",
    "        squared_norm: float = w.dot(w)\n",
    "\n",
    "        return sum_result / self.sample_size + squared_norm * self.lambda_term / 2\n",
    "\n",
    "    def get_gradient(\n",
    "        self, w: np.ndarray, indices: Optional[list[int]] = None\n",
    "    ) -> np.ndarray:\n",
    "        sum_result = np.zeros(self.grad_shape)\n",
    "        for x, y in self._get_iterate_data(indices):\n",
    "            custom_exp = self._custom_exponent(x, y, w)\n",
    "            sum_result += (-y * custom_exp * x) / (1 + custom_exp)\n",
    "\n",
    "        return sum_result / self.sample_size + self.lambda_term * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blr_function.get_value(w)=176.55849319150303\n",
      "blr_function.get_gradient(w).shape=(13,)\n"
     ]
    }
   ],
   "source": [
    "blr_lambda, nlr_mu, blr_lipschitz = BinaryLogisticRegression.calculate_parameters(\n",
    "    1e-3, train_values\n",
    ")\n",
    "blr_function = BinaryLogisticRegression(train_values, train_targets, blr_lambda)\n",
    "\n",
    "w = np.ones(train_values.shape[1])\n",
    "\n",
    "print(f\"{blr_function.get_value(w)=}\")\n",
    "print(f\"{blr_function.get_gradient(w).shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum iterations: 1000\n",
    "\n",
    "Required precision $\\varepsilon  =10^{-5}$\n",
    "\n",
    "$\\frac{\\| \\nabla f(x^k) \\|}{\\| \\nabla f(x^0) \\|}$ is used as default convergence criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(MANUAL_SEED)\n",
    "START_POINT = np.random.randn(train_values.shape[1])\n",
    "\n",
    "NUM_ITERATIONS = 1000\n",
    "EPSILON = 1e-5\n",
    "\n",
    "\n",
    "def default_criterion(grad_0: np.ndarray, grad_current: np.ndarray) -> float:\n",
    "    return float(np.linalg.norm(grad_current) / np.linalg.norm(grad_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Method:\n",
    "    def _predict(self, w: np.ndarray, x: np.ndarray) -> float:\n",
    "        # As y can be 1 and -1 only, we are interested only in sign\n",
    "        return np.sign(w.dot(x))\n",
    "\n",
    "    def _log_accuracy(self) -> float:\n",
    "        accuracy = 0\n",
    "        if not self.accuracy_logging:\n",
    "            return accuracy\n",
    "\n",
    "        predictions = np.array([self._predict(self.w_curr, x) for x in self.test_values])\n",
    "\n",
    "        if len(predictions) == 0:\n",
    "            self.accuracy_logs.append(accuracy)\n",
    "            return accuracy\n",
    "\n",
    "        accuracy = (predictions == self.test_targets).sum() / self.test_targets.shape[0]\n",
    "        self.accuracy_logs.append(accuracy)\n",
    "        return accuracy\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        f: Problem,\n",
    "        w_0: np.ndarray,\n",
    "        iterations: int,\n",
    "        epsilon: float = EPSILON,\n",
    "        verbose: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"Method base class\n",
    "\n",
    "        Args:\n",
    "            name (str): Method name\n",
    "            f (Problem)\n",
    "            x_0 (np.ndarray): starting point\n",
    "            iterations (int): number of iterations\n",
    "            epsilon (float):Required precision. Defaults to EPSILON\n",
    "            verbose (bool): Defaults to False\n",
    "        \"\"\"\n",
    "\n",
    "        self.name = name\n",
    "        self.f = f\n",
    "        self.w_0 = w_0.copy()\n",
    "        self.iterations = iterations\n",
    "        self.epsilon = epsilon\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.accuracy_logging = False\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def enable_accuracy_logging(self, test_values: np.ndarray, test_targets: np.ndarray):\n",
    "        self.accuracy_logging = True\n",
    "        self.test_values = test_values\n",
    "        self.test_targets = test_targets\n",
    "\n",
    "    def disable_accuracy_logging(self):\n",
    "        self.accuracy_logging = False\n",
    "\n",
    "    def predict(self, values: np.ndarray) -> np.ndarray:\n",
    "        return np.array([self._predict(self.w_sol, x) for x in values])\n",
    "\n",
    "    def get_name(self) -> str:\n",
    "        return self.name\n",
    "\n",
    "    def get_solution(self) -> np.ndarray:\n",
    "        return self.w_sol\n",
    "\n",
    "    def get_iterations(self) -> int:\n",
    "        return self.iterations\n",
    "\n",
    "    def get_pass_iterations(self) -> int:\n",
    "        return self.pass_iterations\n",
    "\n",
    "    def get_time_logs(self) -> list[float]:\n",
    "        return self.time_logs\n",
    "\n",
    "    def get_accumulated_time_logs(self) -> list[float]:\n",
    "        return list(accumulate(self.time_logs, operator.add))\n",
    "\n",
    "    def get_accuracy_logs(self) -> list[float]:\n",
    "        return self.accuracy_logs\n",
    "\n",
    "    def get_criterion_logs(self) -> list[float]:\n",
    "        return self.criterion_logs\n",
    "\n",
    "    def update(self, iteration: int) -> bool:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def calculate_criterion(self, k: int) -> float:\n",
    "        grad_current = self.f.get_gradient(self.w_curr)\n",
    "        return default_criterion(self.grad_0, grad_current)\n",
    "\n",
    "    def reset(self):\n",
    "        self.criterion_logs = []\n",
    "        self.time_logs = []\n",
    "        self.accuracy_logs = []\n",
    "\n",
    "        self.pass_iterations = 0\n",
    "\n",
    "        self.w_prev = self.w_0.copy()\n",
    "        self.w_curr = self.w_0.copy()\n",
    "        self.w_sol = self.w_0.copy()\n",
    "        self.grad_0 = self.f.get_gradient(self.w_0)\n",
    "\n",
    "    def start(self) -> np.ndarray:\n",
    "        \"\"\"Start iterating\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: solution\n",
    "        \"\"\"\n",
    "        self.reset()\n",
    "\n",
    "        loop = range(self.iterations)\n",
    "        if not self.verbose:\n",
    "            loop = tqdm(range(self.iterations), desc=self.name, leave=True, position=0)\n",
    "\n",
    "        for k in loop:\n",
    "            start_time = time.time()\n",
    "            succeeded = self.update(k)\n",
    "            finish_time = time.time()\n",
    "\n",
    "            if not succeeded:  # diverges\n",
    "                break\n",
    "\n",
    "            self.pass_iterations += 1\n",
    "\n",
    "            # logging\n",
    "            elapsed_time = finish_time - start_time\n",
    "            criterion_value = self.calculate_criterion(k)\n",
    "            self.time_logs.append(elapsed_time)\n",
    "            self.criterion_logs.append(criterion_value)\n",
    "            accuracy = self._log_accuracy()\n",
    "\n",
    "            if not self.verbose:\n",
    "                info_dict = {\"time\": elapsed_time, \"criterion\": criterion_value}\n",
    "                if self.accuracy_logging:\n",
    "                    info_dict.update({\"accuracy\": accuracy})\n",
    "                loop.set_postfix(info_dict)  # type: ignore\n",
    "\n",
    "            if np.isnan(criterion_value):  # diverges\n",
    "                break\n",
    "            if criterion_value < self.epsilon:\n",
    "                break\n",
    "\n",
    "        self.w_sol = self.w_curr.copy()\n",
    "\n",
    "        return self.w_sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loopless SVRG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSVRG(Method):\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        f: Problem,\n",
    "        w_0: np.ndarray,\n",
    "        iterations: int,\n",
    "        step_size: Callable[[int], float],\n",
    "        probability: float,\n",
    "        epsilon: float,\n",
    "        verbose: bool,\n",
    "    ) -> None:\n",
    "        super().__init__(name, f, w_0, iterations, epsilon, verbose)\n",
    "        self.step_size = step_size\n",
    "        self.probability = probability\n",
    "\n",
    "    def reset(self):\n",
    "        super().reset()\n",
    "        self.v_curr = self.w_0.copy()  # w in paper\n",
    "        self.v_grad: np.ndarray = self.f.get_gradient(\n",
    "            self.v_curr\n",
    "        )  # full gradient in w_curr\n",
    "\n",
    "    def get_g(self) -> np.ndarray:\n",
    "        random_idx = self.f.get_uniformly_sampled_indices()\n",
    "        return (\n",
    "            self.f.get_gradient(self.w_curr, random_idx)\n",
    "            - self.f.get_gradient(self.w_curr, random_idx)\n",
    "            + self.v_grad\n",
    "        )\n",
    "\n",
    "    def update_v(self):\n",
    "        if np.random.random() < self.probability:\n",
    "            self.v_curr = self.w_curr.copy()\n",
    "            self.v_grad = self.f.get_gradient(self.v_curr)\n",
    "\n",
    "    def update(self, iteration: int) -> bool:\n",
    "        w_next = self.w_curr - self.step_size(iteration) * self.get_g()\n",
    "        self.update_v()\n",
    "\n",
    "        self.w_curr = w_next.copy()\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDEFINED_COLORS = [\n",
    "    \"#ff0000\",\n",
    "    \"#ffd700\",\n",
    "    \"#00ff00\",\n",
    "    \"#04eafc\",\n",
    "    \"#071efb\",\n",
    "    \"#9603fd\",\n",
    "]\n",
    "\n",
    "\n",
    "def draw_method_plots(\n",
    "    methods: list[Method],\n",
    "    title: str,\n",
    "    ylim=None,\n",
    "    figsize=(16, 12),\n",
    "    use_rainbow: bool = False,\n",
    "):\n",
    "    if use_rainbow:\n",
    "        num_colors = len(methods)\n",
    "        cm = plt.get_cmap(\"gist_rainbow\")\n",
    "        colors = [cm(1.0 * i / num_colors) for i in range(num_colors)]\n",
    "    else:\n",
    "        colors = PREDEFINED_COLORS\n",
    "\n",
    "    style_cycler = cycler(linestyle=[\"-\", \"--\", \":\", \"-.\"]) * cycler(color=colors)\n",
    "\n",
    "    _, axs = plt.subplots(3, 1, figsize=figsize, gridspec_kw={\"height_ratios\": [5, 5, 3]})\n",
    "    (ax1, ax2, ax3) = axs\n",
    "    ax1.set_title(f\"{title} | Criterion / Iteration\")\n",
    "    ax2.set_title(f\"{title} | Criterion / Time\")\n",
    "    ax3.set_title(f\"{title} | Elapsed time\")\n",
    "\n",
    "    ax1.set(xlabel=\"Iteration\", ylabel=\"Criterion value, log scale\")\n",
    "    ax2.set(xlabel=\"Running time, seconds\", ylabel=\"Criterion value, log scale\")\n",
    "    ax3.set(xlabel=\"Iteration\", ylabel=\"Elapsed time\")\n",
    "\n",
    "    ax1.set_yscale(\"log\")\n",
    "    ax2.set_yscale(\"log\")\n",
    "\n",
    "    for ax in axs.flat:\n",
    "        ax.grid()\n",
    "        ax.set_prop_cycle(style_cycler)\n",
    "\n",
    "    for method in methods:\n",
    "        label = method.get_name()\n",
    "\n",
    "        criterion = method.get_criterion_logs()\n",
    "        iterations = range(method.get_pass_iterations())\n",
    "        accumulated_time = method.get_accumulated_time_logs()\n",
    "        elapsed_time = method.get_time_logs()\n",
    "\n",
    "        ax1.plot(iterations, criterion, label=label)\n",
    "        ax1.scatter(iterations[-1], criterion[-1], s=15)\n",
    "\n",
    "        ax2.plot(accumulated_time, criterion, label=label)\n",
    "        ax2.scatter(accumulated_time[-1], criterion[-1], s=15)\n",
    "\n",
    "        ax3.plot(iterations, elapsed_time, label=label)\n",
    "\n",
    "    if len(methods) > 1:\n",
    "        for ax in axs.flat:\n",
    "            ax.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1))\n",
    "\n",
    "    for ax in axs.flat:\n",
    "        if ylim is not None:\n",
    "            ax.set_ylim(top=ylim)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparisons"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omml-project-f23-j1-kYpQ4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
